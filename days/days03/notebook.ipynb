{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e1673d",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cf0c9",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70bd5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/rl6y80hd7tq9c91b7bx47m640000gn/T/ipykernel_83073/4121823300.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\" : \"Hi\"}, {\"output\" : \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2bae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\" : \"Hi\"}, {\"output\" : \"How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b780e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\" : \"Hi\"}, {\"output\" : \"How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e822224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527d139",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4065befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/rl6y80hd7tq9c91b7bx47m640000gn/T/ipykernel_83073/905321663.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\" : input}, {\"output\" : output})\n",
    "\n",
    "add_message(\"1\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c246fff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='0', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='0', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='1', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    add_message(str(i), str(i))\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2a9c4",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607712dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/rl6y80hd7tq9c91b7bx47m640000gn/T/ipykernel_83073/2623785103.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0.1)\n",
      "/var/folders/mh/rl6y80hd7tq9c91b7bx47m640000gn/T/ipykernel_83073/2623785103.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\" : input}, {\"output\" : output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Jaehun, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ba2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Sung-il information high school is so suck\", \"I wish I could go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e76dfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Jaehun from South Korea introduces himself to the AI, who responds enthusiastically. Jaehun mentions Sung-il Information High School is not great, and the AI expresses a wish to attend.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc5e7a",
   "metadata": {},
   "source": [
    "## ConversationKGMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5cc9939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m88 packages\u001b[0m \u001b[2min 436ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m     0 B/1.94 MiB            \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 14.89 KiB/1.94 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 30.89 KiB/1.94 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 46.89 KiB/1.94 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 62.89 KiB/1.94 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 78.89 KiB/1.94 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 94.89 KiB/1.94 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 110.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 126.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 142.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 158.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 174.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 190.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 206.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 222.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 222.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 238.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 254.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 270.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 286.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 302.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 318.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 334.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 350.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 350.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 382.89 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 392.56 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 408.56 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 424.56 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 472.56 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 488.56 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 536.56 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 552.56 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 600.56 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 616.56 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 658.38 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 674.38 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 722.38 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 786.38 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 882.38 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 930.38 KiB/1.94 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1010.38 KiB/1.94 MiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.02 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)m-------------\u001b[0m\u001b[0m 1.08 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)[2m-----------\u001b[0m\u001b[0m 1.18 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\u001b[2m----------\u001b[0m\u001b[0m 1.26 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-\u001b[2m---------\u001b[0m\u001b[0m 1.34 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--\u001b[2m--------\u001b[0m\u001b[0m 1.40 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---\u001b[2m-------\u001b[0m\u001b[0m 1.45 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-----\u001b[2m-----\u001b[0m\u001b[0m 1.56 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)------\u001b[2m----\u001b[0m\u001b[0m 1.64 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------\u001b[2m---\u001b[0m\u001b[0m 1.71 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------\u001b[2m--\u001b[0m\u001b[0m 1.77 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---------\u001b[2m-\u001b[0m\u001b[0m 1.83 MiB/1.94 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 1.91s\u001b[0m\u001b[0m                                                  \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2987bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\" : input}, {\"output\" : output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84a7dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_message(\"Hi I'm Jaehun, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e96ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Jaehun: Jaehun lives in South Korea.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\" : \"who is Jaehun?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508834e",
   "metadata": {},
   "source": [
    "## Memory on LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748a2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant city with a rich history and a mix of modern and traditional culture. What do you enjoy most about living there?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLm = Off the shlf : General Perpose chain (일반적인 목적을 가지는 체인)\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    "    memory_key=\"chat_history\"\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "    you are a helpful AI taling a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    Your:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    # prompt=PromptTemplate.from_template(\"{question}\")\n",
    "    prompt=PromptTemplate.from_template(template=template)\n",
    ")\n",
    "\n",
    "chain.predict(question=\"I live in seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5424ce87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI: Since you mentioned you live in Seoul, you might be in one of its many districts, like Gangnam, Hongdae, or Insadong. Each area has its own unique charm and attractions. Do you have a specific neighborhood in mind?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"Where's do I live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2493f3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"System: The human mentions living in Seoul. The AI responds positively, highlighting Seoul's vibrant culture and history, and asks what the human enjoys most about living there.\\nHuman: Where's do I live?\\nAI: AI: Since you mentioned you live in Seoul, you might be in one of its many districts, like Gangnam, Hongdae, or Insadong. Each area has its own unique charm and attractions. Do you have a specific neighborhood in mind?\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920771b",
   "metadata": {},
   "source": [
    "## ChatBasedMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ba1a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: Where's do I live?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals, including where you live. If you have any other questions or need assistance with something else, feel free to ask!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"Where's do I live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7723cb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: Where's do I live?\n",
      "AI: I'm sorry, but I don't have access to personal information about individuals, including where you live. If you have any other questions or need assistance with something else, feel free to ask!\n",
      "Human: Where's do I live?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I still can't determine where you live. If you provide me with some context or details, I might be able to help you with related information or answer any questions you have!\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"Where's do I live?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea156d",
   "metadata": {},
   "source": [
    "## LCEL Based Memory (LangChain Expressrion Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\" : question\n",
    "    })\n",
    "    memory.save_context({\"input\" : question}, {\"output\" : result.content})\n",
    "    print(result)\n",
    "# chain.invoke(\n",
    "#     {\n",
    "#         # \"chat_history\" : memory.load_memory_variables({})[\"chat_history\"],\n",
    "#         \"chat_history\" : load_memory(),\n",
    "#         \"question\" : \"My name is Jaehun\"\n",
    "#         }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0755cfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, Jaehun! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 26, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CKhc7Jq4SyD6jINwfASgFgL6vw2RO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--d591b31b-ba7c-4627-867d-7f15cd1e17a6-0' usage_metadata={'input_tokens': 26, 'output_tokens': 16, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is Jaehun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9402b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Jaehun. How can I help you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CKhcQlLuWHWrZeOuHs7lwIdCk2K1J', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--ad5da7ec-9b88-4183-9e6c-7333663eae23-0' usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
